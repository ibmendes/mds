{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f028f1",
   "metadata": {},
   "source": [
    "# Iniciar sessão Spark com suporte a Hive e Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2081e9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "findspark.init()\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971adb0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark versão: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark versão:\", spark.version)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35624f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scala: 2.13.16\n"
     ]
    }
   ],
   "source": [
    "print(\"scala:\", spark.sparkContext._jvm.scala.util.Properties.versionNumberString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b7b0af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a2fc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: delta-spark\n",
      "Version: 4.0.0\n",
      "Summary: Python APIs for using Delta Lake with Apache Spark\n",
      "Home-page: https://github.com/delta-io/delta/\n",
      "Author: The Delta Lake Project Authors\n",
      "Author-email: delta-users@googlegroups.com\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: importlib-metadata, pyspark\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ceae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark versão: 4.0.0\n",
      "Suporte a Hive ativado?: hive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 40172)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 1.1. Importar tudo o que precisamos\n",
    "from pyspark.sql import SparkSession\n",
    "from delta import configure_spark_with_delta_pip\n",
    "\n",
    "# 1.2. Construir o SparkSession com Hive e Delta ativados\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"TesteSparkHiveDelta\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive:9083\")\n",
    "    # .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    # .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # .config(\"spark.hadoop.hive.server2.thrift.max.message.size\", \"104857600\")\n",
    "    # .config(\"spark.jars.packages\", \"io.delta:delta-core_2.13:4.0.0rc1\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 1.3. Verificar versão do Spark e se o Hive está disponível\n",
    "print(\"Spark versão:\", spark.version)\n",
    "print(\"Suporte a Hive ativado?:\", spark.conf.get(\"spark.sql.catalogImplementation\"))  # deve imprimir \"hive\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e940c1f3",
   "metadata": {},
   "source": [
    "# 2.Teste básico de leitura/escrita de DataFrame local\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502b15f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- idade: long (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-----+\n",
      "| id|idade| nome|\n",
      "+---+-----+-----+\n",
      "|  1|   30|Alice|\n",
      "|  2|   25|  Bob|\n",
      "|  3|   40|Carol|\n",
      "+---+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.1. Criar um DataFrame simples\n",
    "data = [\n",
    "    {\"id\": 1, \"nome\": \"Alice\", \"idade\": 30},\n",
    "    {\"id\": 2, \"nome\": \"Bob\",   \"idade\": 25},\n",
    "    {\"id\": 3, \"nome\": \"Carol\", \"idade\": 40}\n",
    "]\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# 2.2. Mostrar esquema e conteúdo\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47231624",
   "metadata": {},
   "source": [
    "# 3. Conectar ao Hive e criar banco/tabela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15916599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thrift://hive:9083'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._jsc.hadoopConfiguration().get(\"hive.metastore.uris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273418cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hive'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71843b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://namenode:9000/user/hive/warehouse'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2739ecef",
   "metadata": {},
   "source": [
    "## 3.1. Cria um database no Hive (caso não exista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2283149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 23:51:21 WARN HiveConf: HiveConf of name hive.server2.thrift.jvm.args does not exist\n",
      "25/06/14 23:51:21 WARN HiveConf: HiveConf of name hive.server2.thrift.java.port does not exist\n",
      "Hive Session ID = 1df1b4a2-3bf6-4679-8e5f-8fd57ff3fcfe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53a1375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# propriedade do Hive Metastore (java api)\n",
    "warehouse_dir = spark.conf.get(\"spark.sql.warehouse.dir\")\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "path = spark._jvm.org.apache.hadoop.fs.Path(warehouse_dir)\n",
    "\n",
    "if fs.exists(path):\n",
    "    status = fs.listStatus(path)\n",
    "    for file_status in status:\n",
    "        print(file_status.getPath())\n",
    "else:\n",
    "    print(f\"{warehouse_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "076ed1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://namenode:9000/tmp\n",
      "hdfs://namenode:9000/user\n"
     ]
    }
   ],
   "source": [
    "# hdfs root\n",
    "root_path = spark._jvm.org.apache.hadoop.fs.Path(\"/\")\n",
    "status = fs.listStatus(root_path)\n",
    "for file_status in status:\n",
    "    print(file_status.getPath())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072b975",
   "metadata": {},
   "source": [
    "#### importante: hdfs =! metastore\n",
    "aqui estamos tentando validar a conexão com o namenode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19170942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwx-wx-wx   - hdfs supergroup          0 2025-06-14 23:50 hdfs://namenode:9000/tmp\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:50 hdfs://namenode:9000/user\n"
     ]
    }
   ],
   "source": [
    "!HADOOP_USER_NAME=sparkuser hdfs dfs -ls hdfs://namenode:9000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00c66a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwx-wx-wx   - hdfs supergroup          0 2025-06-14 23:50 /tmp\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:50 /user\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0929428e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS spark_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b89d4974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thrift://hive:9083'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.hadoop.hive.metastore.uris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de066e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hdfs://namenode:9000/user/hive/warehouse'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.warehouse.dir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4993dbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b26d06db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| spark_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33ae44",
   "metadata": {},
   "source": [
    "## 3.2. Cria uma tabela Hive que grava em formato Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16208bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| spark_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.get(\"spark.hadoop.hive.metastore.uris\")\n",
    "spark.conf.get(\"spark.sql.catalogImplementation\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3953963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current database: spark_db\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"USE spark_db\")\n",
    "\n",
    "print(\"Current database:\", spark.catalog.currentDatabase())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22ef1bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get(\"HADOOP_USER_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bc3510a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:51 /user/hive/warehouse/spark_db.db\n"
     ]
    }
   ],
   "source": [
    "# !hdfs dfs -mkdir -p /user/hive/warehouse\n",
    "# !hdfs dfs -chown -R hive:hive /user/hive\n",
    "# !hdfs dfs -chmod -R 775 /user/hive\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec0669f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TESTE\n",
    "# spark.sql(\"drop table if exists spark_db.tabela_hive_parquet PURGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156d44ca",
   "metadata": {},
   "source": [
    "## obs, ignore o primeiro warn, ele cria o diretorio e tras o aviso. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a352675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS spark_db.tabela_hive_parquet (\n",
    "    id INT,\n",
    "    nome STRING,\n",
    "    idade INT\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://namenode:9000/user/hive/warehouse/spark_db.db/tabela_hive_parquet'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e2c10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:51 /user/hive/warehouse/spark_db.db\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31a7bf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-----------+\n",
      "|namespace|          tableName|isTemporary|\n",
      "+---------+-------------------+-----------+\n",
      "| spark_db|tabela_hive_parquet|      false|\n",
      "+---------+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN spark_db\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6cfc3e",
   "metadata": {},
   "source": [
    "## 3.3. Insere alguns dados na tabela Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3dabe28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                               |comment|\n",
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                     |NULL   |\n",
      "|nome                        |string                                                                  |NULL   |\n",
      "|idade                       |int                                                                     |NULL   |\n",
      "|                            |                                                                        |       |\n",
      "|# Detailed Table Information|                                                                        |       |\n",
      "|Catalog                     |spark_catalog                                                           |       |\n",
      "|Database                    |spark_db                                                                |       |\n",
      "|Table                       |tabela_hive_parquet                                                     |       |\n",
      "|Owner                       |hdfs                                                                    |       |\n",
      "|Created Time                |Sat Jun 14 23:51:26 UTC 2025                                            |       |\n",
      "|Last Access                 |UNKNOWN                                                                 |       |\n",
      "|Created By                  |Spark 4.0.0                                                             |       |\n",
      "|Type                        |EXTERNAL                                                                |       |\n",
      "|Provider                    |hive                                                                    |       |\n",
      "|Table Properties            |[bucketing_version=2, transient_lastDdlTime=1749945086]                 |       |\n",
      "|Location                    |hdfs://namenode:9000/user/hive/warehouse/spark_db.db/tabela_hive_parquet|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe             |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat           |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat          |       |\n",
      "|Storage Properties          |[serialization.format=1]                                                |       |\n",
      "+----------------------------+------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE FORMATTED spark_db.tabela_hive_parquet\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c6e9f1",
   "metadata": {},
   "source": [
    "### troubleshotting \n",
    "1. block de insert -> desativar o safe mode do hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655d4274",
   "metadata": {},
   "source": [
    "## 3.4. Verifica se os dados foram gravados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b09a93",
   "metadata": {},
   "source": [
    "# 4. Operações “HDFS” (via API Hadoop) — criar diretórios e mover arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6bf2d0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1. Importar classes do Hadoop FileSystem via o gateway Java (JVM)\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83b470db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2. Definir caminhos para diretório e arquivos de teste no HDFS local\n",
    "\n",
    "path_dir = spark._jvm.org.apache.hadoop.fs.Path(\"/tmp/hdfs_exemplo_dir\")\n",
    "path_arquivo_origem = spark._jvm.org.apache.hadoop.fs.Path(\"/tmp/hdfs_exemplo_dir/original.txt\")\n",
    "path_arquivo_destino = spark._jvm.org.apache.hadoop.fs.Path(\"/tmp/hdfs_exemplo_dir/movido.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1d620de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório criado: /tmp/hdfs_exemplo_dir\n"
     ]
    }
   ],
   "source": [
    "# 4.3. Criar o diretório (se não existir)\n",
    "if not fs.exists(path_dir):\n",
    "    fs.mkdirs(path_dir)\n",
    "    print(\"Diretório criado:\", path_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c358413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo de origem criado: /tmp/hdfs_exemplo_dir/original.txt\n"
     ]
    }
   ],
   "source": [
    "# 4.4. Criar um arquivo “dummy” no diretório para testar a movimentação\n",
    "#      Para “criar” um arquivo, usamos o FileSystem API para abrir um OutputStream\n",
    "os_stream = fs.create(path_arquivo_origem, True)\n",
    "os_stream.write(bytes(\"Conteúdo de teste HDFS\\n\", \"utf-8\"))\n",
    "os_stream.close()\n",
    "print(\"Arquivo de origem criado:\", path_arquivo_origem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ca398be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe original.txt? True\n"
     ]
    }
   ],
   "source": [
    "# 4.5. Verificar se o arquivo existe\n",
    "print(\"Existe original.txt?\", fs.exists(path_arquivo_origem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7a67ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo movido para: /tmp/hdfs_exemplo_dir/movido.txt\n"
     ]
    }
   ],
   "source": [
    "# 4.6. Mover (renomear) o arquivo de original.txt → movido.txt\n",
    "fs.rename(path_arquivo_origem, path_arquivo_destino)\n",
    "print(\"Arquivo movido para:\", path_arquivo_destino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80715f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existe movido.txt? True\n",
      "Existe original.txt (depois do rename)? True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4.7. Conferir novamente\n",
    "print(\"Existe movido.txt?\", fs.exists(path_arquivo_destino))\n",
    "print(\"Existe original.txt (depois do rename)?\", fs.exists(path_arquivo_destino))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605278b1",
   "metadata": {},
   "source": [
    "# 5. Criar tabelas Hive apontando para esse “HDFS” (diretório local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0c64f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diretório para tabela externa criado: hdfs://namenode:9000/hdfs_exemplo_dir/tabela_externa_hive\n"
     ]
    }
   ],
   "source": [
    "# 5.1. Define um diretório “externo” para a tabela Hive\n",
    "dir_externo = \"hdfs://namenode:9000/hdfs_exemplo_dir/tabela_externa_hive\"\n",
    "\n",
    "# 5.2. Caso não exista, cria o diretório (já criamos antes, mas só para garantir)\n",
    "path_dir_externo = spark._jvm.org.apache.hadoop.fs.Path(dir_externo)\n",
    "if not fs.exists(path_dir_externo):\n",
    "    fs.mkdirs(path_dir_externo)\n",
    "    print(\"Diretório para tabela externa criado:\", dir_externo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c846c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 5.3. Criar tabela externa no Hive que aponta para dir_externo\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tabela_hive_externa (\n",
    "    id INT,\n",
    "    descricao STRING\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{dir_externo}'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d45397a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|         descricao|\n",
      "+---+------------------+\n",
      "|100|Registro Externo A|\n",
      "|101|Registro Externo B|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5.4. Inserir alguns dados nessa tabela externa\n",
    "spark.sql(\"INSERT INTO tabela_hive_externa VALUES (100, 'Registro Externo A'), (101, 'Registro Externo B')\")\n",
    "\n",
    "# 5.5. Verificar conteúdo via SELECT\n",
    "spark.sql(\"SELECT * FROM tabela_hive_externa\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8708753",
   "metadata": {},
   "source": [
    "# 6. Ler/Escrever diretamente arquivos CSV/Parquet via Spark (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8ffa06cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV gravado em: hdfs://namenode:9000/hdfs_exemplo_dir/produtos.csv\n"
     ]
    }
   ],
   "source": [
    "# 6.1. Cria um DataFrame e grava como CSV\n",
    "df_csv = spark.createDataFrame([\n",
    "    (1,   \"Produto A\",  9.99),\n",
    "    (2,   \"Produto B\", 19.99),\n",
    "    (3,   \"Produto C\", 29.99)\n",
    "], [\"produto_id\", \"produto_nome\", \"preco\"])\n",
    "\n",
    "caminho_csv = \"hdfs://namenode:9000/hdfs_exemplo_dir/produtos.csv\"\n",
    "df_csv.write.mode(\"overwrite\").csv(caminho_csv, header=True)\n",
    "\n",
    "print(\"CSV gravado em:\", caminho_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb0f5dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- produto_id: string (nullable = true)\n",
      " |-- produto_nome: string (nullable = true)\n",
      " |-- preco: string (nullable = true)\n",
      "\n",
      "+----------+------------+-----+\n",
      "|produto_id|produto_nome|preco|\n",
      "+----------+------------+-----+\n",
      "|         2|   Produto B|19.99|\n",
      "|         3|   Produto C|29.99|\n",
      "|         1|   Produto A| 9.99|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6.2. Lê o CSV que acabamos de gravar\n",
    "df_csv_lido = spark.read.option(\"header\", True).csv(caminho_csv)\n",
    "df_csv_lido.printSchema()\n",
    "df_csv_lido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "45ee553f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet gravado em: hdfs://namenode:9000/hdfs_exemplo_dir/produtos_parquet\n",
      "root\n",
      " |-- produto_id: string (nullable = true)\n",
      " |-- produto_nome: string (nullable = true)\n",
      " |-- preco: string (nullable = true)\n",
      "\n",
      "+----------+------------+-----+\n",
      "|produto_id|produto_nome|preco|\n",
      "+----------+------------+-----+\n",
      "|         2|   Produto B|19.99|\n",
      "|         3|   Produto C|29.99|\n",
      "|         1|   Produto A| 9.99|\n",
      "+----------+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6.3. Converte para Parquet e grava em diretório separado\n",
    "caminho_parquet = \"hdfs://namenode:9000/hdfs_exemplo_dir/produtos_parquet\"\n",
    "df_csv_lido.write.mode(\"overwrite\").parquet(caminho_parquet)\n",
    "\n",
    "print(\"Parquet gravado em:\", caminho_parquet)\n",
    "\n",
    "# 6.4. Lê o Parquet de volta\n",
    "df_parquet_lido = spark.read.parquet(caminho_parquet)\n",
    "df_parquet_lido.printSchema()\n",
    "df_parquet_lido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90e5490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:51 /hdfs_exemplo_dir\n",
      "drwx-wx-wx   - hdfs supergroup          0 2025-06-14 23:51 /tmp\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:50 /user\n"
     ]
    }
   ],
   "source": [
    "# persist? \n",
    "!hdfs dfs -ls /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "326636a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist? \n",
    "!hdfs dfs -ls /user/hive/warehouse/spark_db.db/tabela_hive_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34364d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - hdfs supergroup          0 2025-06-14 23:51 /user/hive/warehouse/spark_db.db\n"
     ]
    }
   ],
   "source": [
    "# persist? \n",
    "!hdfs dfs -ls /user/hive/warehouse/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cc1c4",
   "metadata": {},
   "source": [
    "# 7. Testar leitura/escrita de Delta Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08ce0b60",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "84692439",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MyApp\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive:9083\")\n",
    ")\n",
    "# Não uso configure_spark_with_delta_pip(), pois ele adiciona --packages\n",
    "\n",
    "spark = builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8abf680b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 JARs com 'delta' encontrados no classpath:\n",
      "\n",
      "- /opt/spark/jars/delta-spark_2.13-4.0.0.jar\n",
      "- /opt/spark/jars/delta-storage-4.0.0.jar\n",
      "- /opt/spark/jars/delta-core_2.13-2.4.0.jar\n",
      "\n",
      "2 JARs com 'antlr' encontrados no classpath:\n",
      "\n",
      "- /opt/spark/jars/antlr4-runtime-4.13.1.jar\n",
      "- /opt/spark/jars/antlr-runtime-3.5.2.jar\n"
     ]
    }
   ],
   "source": [
    "# Acessa o classpath da JVM via SparkContext\n",
    "classpath = spark.sparkContext._jvm.System.getProperty(\"java.class.path\")\n",
    "\n",
    "# Divide os caminhos (separados por ':' no Linux/macOS, ou ';' no Windows)\n",
    "import os\n",
    "sep = \";\" if os.name == \"nt\" else \":\"\n",
    "jars = [path for path in classpath.split(sep) if path.strip().endswith(\".jar\")]\n",
    "\n",
    "# Filtra JARs que contenham \"delta\" no nome (case-insensitive)\n",
    "delta_jars = [jar for jar in jars if \"delta\" in jar.lower()]\n",
    "antlr_jars = [jar for jar in jars if \"antlr\" in jar.lower()]\n",
    "\n",
    "# Exibe os JARs encontrados\n",
    "print(f\"{len(delta_jars)} JARs com 'delta' encontrados no classpath:\\n\")\n",
    "for jar in delta_jars:\n",
    "    print(\"-\", jar)\n",
    "print(f\"\\n{len(antlr_jars)} JARs com 'antlr' encontrados no classpath:\\n\")\n",
    "for jar in antlr_jars:\n",
    "    print(\"-\", jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e04cdd4f",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 4.0.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: py4j\n",
      "Required-by: delta-spark\n",
      "---\n",
      "Name: delta-spark\n",
      "Version: 4.0.0\n",
      "Summary: Python APIs for using Delta Lake with Apache Spark\n",
      "Home-page: https://github.com/delta-io/delta/\n",
      "Author: The Delta Lake Project Authors\n",
      "Author-email: delta-users@googlegroups.com\n",
      "License: Apache-2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: importlib-metadata, pyspark\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark delta-spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a3ccb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 7.2. Definir diretório para tabela Delta\n",
    "delta_path = \"hdfs://namenode:9000/tmp/delta_exemplo\"\n",
    "# Se já existir, apagamos para recomeçar do zero\n",
    "fs_local = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "path_delta_dir = spark._jvm.org.apache.hadoop.fs.Path(delta_path)\n",
    "if fs_local.exists(path_delta_dir):\n",
    "    fs_local.delete(path_delta_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bdc1e",
   "metadata": {},
   "source": [
    "## builder helper (usado em testes para achar os jars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fa72f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark\n",
    "# from delta import *\n",
    "\n",
    "# builder = pyspark.sql.SparkSession.builder.appName(\"MyApp\") \\\n",
    "#     .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "#     .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "# spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "49524371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|categoria| id|valor|\n",
      "+---------+---+-----+\n",
      "|        A|  1|  100|\n",
      "|        B|  2|  200|\n",
      "|        C|  3|  300|\n",
      "+---------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.1. Criar um DataFrame inicial\n",
    "data_delta = [\n",
    "    {\"id\": 1, \"categoria\": \"A\", \"valor\": 100},\n",
    "    {\"id\": 2, \"categoria\": \"B\", \"valor\": 200},\n",
    "    {\"id\": 3, \"categoria\": \"C\", \"valor\": 300}\n",
    "]\n",
    "df_delta = spark.createDataFrame(data_delta)\n",
    "df_delta.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "859e0658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame salvo em Delta no caminho: hdfs://namenode:9000/tmp/delta_sample\n"
     ]
    }
   ],
   "source": [
    "# Definir caminho para salvar em Delta (pode sobrescrever delta_path se desejar)\n",
    "delta_path = \"hdfs://namenode:9000/tmp/delta_sample\"\n",
    "\n",
    "# Se já existir, apagar para evitar erro\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "path_sample_delta = spark._jvm.org.apache.hadoop.fs.Path(delta_path)\n",
    "if fs.exists(path_sample_delta):\n",
    "    fs.delete(path_sample_delta, True)\n",
    "\n",
    "# Gravar o DataFrame em formato Delta\n",
    "df_delta.write.format(\"delta\").mode(\"overwrite\").save(delta_path)\n",
    "print(f\"DataFrame salvo em Delta no caminho: {delta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "564ebda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo inicial do Delta:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 23:51:39 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|categoria| id|valor|\n",
      "+---------+---+-----+\n",
      "|        A|  1|  100|\n",
      "|        B|  2|  200|\n",
      "|        C|  3|  300|\n",
      "+---------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7.4. Ler o Delta que acabamos de gravar\n",
    "df_lido = spark.read.format(\"delta\").load(delta_path)\n",
    "print(\"Conteúdo inicial do Delta:\")\n",
    "df_lido.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3f0bf7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histórico de versões (history):\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2025-06-14 23:51:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7.5. Exibe o histórico de versões (Timeline) do Delta\n",
    "from delta.tables import DeltaTable\n",
    "delta_table = DeltaTable.forPath(spark, delta_path)\n",
    "print(\"Histórico de versões (history):\")\n",
    "delta_table.history().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fdcd81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|categoria| id|valor|\n",
      "+---------+---+-----+\n",
      "|        B|  2|  250|\n",
      "|        D|  4|  400|\n",
      "+---------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7.6. Simular um “upsert” (merge) — exemplo: atualiza valor onde id=2, insere novo registro id=4\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Cria um DataFrame com alterações/novos registros\n",
    "data_upsert = [\n",
    "    {\"id\": 2, \"categoria\": \"B\", \"valor\": 250},  # atualizar\n",
    "    {\"id\": 4, \"categoria\": \"D\", \"valor\": 400}   # novo\n",
    "]\n",
    "df_upsert = spark.createDataFrame(data_upsert)\n",
    "\n",
    "df_upsert.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "50d6ec36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 23:51:43 WARN MapPartitionsRDD: RDD 58 was locally checkpointed, its lineage has been truncated and cannot be recomputed after unpersisting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conteúdo do Delta após MERGE:\n",
      "+---------+---+-----+\n",
      "|categoria| id|valor|\n",
      "+---------+---+-----+\n",
      "|        B|  2|  250|\n",
      "|        D|  4|  400|\n",
      "|        A|  1|  100|\n",
      "|        C|  3|  300|\n",
      "+---------+---+-----+\n",
      "\n",
      "Histórico de versões (history) após MERGE:\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      1|2025-06-14 23:51:...|  NULL|    NULL|    MERGE|{predicate -> [\"(...|NULL|    NULL|     NULL|          0|  Serializable|        false|{numTargetRowsCop...|        NULL|Apache-Spark/4.0....|\n",
      "|      0|2025-06-14 23:51:...|  NULL|    NULL|    WRITE|{mode -> Overwrit...|NULL|    NULL|     NULL|       NULL|  Serializable|        false|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Executa o MERGE: se id bate, atualiza; caso contrário, insere\n",
    "delta_table.alias(\"tgt\").merge(\n",
    "    source = df_upsert.alias(\"src\"),\n",
    "    condition = \"tgt.id = src.id\"\n",
    ").whenMatchedUpdate(set = {\"valor\": \"src.valor\"}) \\\n",
    " .whenNotMatchedInsert(values = {\n",
    "     \"id\": \"src.id\",\n",
    "     \"categoria\": \"src.categoria\",\n",
    "     \"valor\": \"src.valor\"\n",
    " }).execute()\n",
    "\n",
    "# 7.7. Mostrar o conteúdo final após o merge\n",
    "print(\"Conteúdo do Delta após MERGE:\")\n",
    "spark.read.format(\"delta\").load(delta_path).show()\n",
    "\n",
    "# 7.8. Mostrar histórico atualizado (nova versão)\n",
    "print(\"Histórico de versões (history) após MERGE:\")\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3c4f9",
   "metadata": {},
   "source": [
    "# 8. (spark-sql) Criar tabela Hive com dados Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1354716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| spark_db|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 23:51:44 WARN HiveConf: HiveConf of name hive.server2.thrift.jvm.args does not exist\n",
      "25/06/14 23:51:44 WARN HiveConf: HiveConf of name hive.server2.thrift.java.port does not exist\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff4174ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS teste_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ffb6092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "| spark_db|\n",
      "| teste_db|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7cc6060d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 23:51:45 WARN HiveExternalCatalog: Couldn't find corresponding Hive SerDe for data source provider delta. Persisting data source table `spark_catalog`.`teste_db`.`tabela_hive_delta` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE teste_db\")  # volta ao banco de dados de teste\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS tabela_hive_delta\")\n",
    "\n",
    "# Cria a tabela Delta no Hive apontando para o delta_path, sem schema explícito\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS tabela_hive_delta\n",
    "USING DELTA\n",
    "LOCATION '{delta_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5aaa27d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----+\n",
      "|categoria| id|valor|\n",
      "+---------+---+-----+\n",
      "|        B|  2|  250|\n",
      "|        D|  4|  400|\n",
      "|        A|  1|  100|\n",
      "|        C|  3|  300|\n",
      "+---------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.2. Ler a tabela Delta via API e registrar como view temporária\n",
    "df_delta_sql = spark.read.format(\"delta\").load(delta_path)\n",
    "df_delta_sql.createOrReplaceTempView(\"tabela_hive_delta_view\")\n",
    "\n",
    "# Agora pode consultar via SQL sem erro\n",
    "spark.sql(\"SELECT * FROM tabela_hive_delta_view\").show()\n",
    "\n",
    "# 8.3. Mostrar histórico a partir do SQL (opcional)\n",
    "# Note: não há “SHOW HISTORY” em SQL padrão; mas podemos\n",
    "# consultar o DeltaTable via Python, conforme feito acima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4486511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|version|timestamp              |userId|userName|operation|operationParameters                                                                                                                                                            |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |userMetadata|engineInfo                         |\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "|1      |2025-06-14 23:51:43.48 |NULL  |NULL    |MERGE    |{predicate -> [\"(id#1278L = id#1408L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|NULL|NULL    |NULL     |0          |Serializable  |false        |{numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetBytesRemoved -> 1010, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, numTargetRowsMatchedDeleted -> 0, numTargetRowsUpdated -> 1, numTargetChangeFilesAdded -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 292, numTargetFilesAdded -> 1, numTargetBytesAdded -> 1023, executionTimeMs -> 1078, materializeSourceTimeMs -> 183, numTargetRowsInserted -> 1, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 596, numOutputRows -> 2, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1}|NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "|0      |2025-06-14 23:51:38.944|NULL  |NULL    |WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                         |NULL|NULL    |NULL     |NULL       |Serializable  |false        |{numFiles -> 4, numRemovedFiles -> 0, numRemovedBytes -> 0, numOutputRows -> 3, numOutputBytes -> 3495}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |NULL        |Apache-Spark/4.0.0 Delta-Lake/4.0.0|\n",
      "+-------+-----------------------+------+--------+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cria uma view temporária com o histórico\n",
    "delta_table.history().createOrReplaceTempView(\"delta_history\")\n",
    "\n",
    "# Agora pode consultar via SQL\n",
    "spark.sql(\"SELECT * FROM delta_history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12875c",
   "metadata": {},
   "source": [
    "# 9 teste sas7 package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "85be2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebbb722",
   "metadata": {},
   "source": [
    "## venv 3.10 + scala 2.12 (mude o kernel manualmente)\n",
    "obs: https://github.com/saurfang/spark-sas7bdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "51588b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(f\"Spark version: {pyspark.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "76ae2d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark versão: 4.0.0\n",
      "Suporte a Hive ativado?: hive\n"
     ]
    }
   ],
   "source": [
    "# 1.1. Importar tudo o que precisamos\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1.2. Construir o SparkSession com Hive\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"sas lab\")\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "    .config(\"spark.hadoop.hive.metastore.uris\", \"thrift://hive:9083\")\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/jars/scala-library-2.12.2.jar:/opt/spark/jars/spark-sas7bdat-3.0.0-s_2.12.jar:/opt/spark/jars/parso-2.0.14.jar\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/jars/scala-library-2.12.2.jar:/opt/spark/jars/spark-sas7bdat-3.0.0-s_2.12.jar:/opt/spark/jars/parso-2.0.14.jar\") \\\n",
    "    # .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    # .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    # .config(\"spark.hadoop.hive.server2.thrift.max.message.size\", \"104857600\")\n",
    "    # .config(\"spark.jars.packages\", \"io.delta:delta-core_2.13:4.0.0rc1\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# 1.3. Verificar versão do Spark e se o Hive está disponível\n",
    "print(\"Spark versão:\", spark.version)\n",
    "print(\"Suporte a Hive ativado?:\", spark.conf.get(\"spark.sql.catalogImplementation\"))  # deve imprimir \"hive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3e63cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 JARs com 'sas' encontrados no classpath:\n",
      "\n",
      "- /opt/spark/jars/spark-sas7bdat-3.0.0-s_2.12.jar\n",
      "\n",
      "1 JARs com 'parso' encontrados no classpath:\n",
      "\n",
      "- /opt/spark/jars/parso-2.0.14.jar\n",
      "\n",
      "1 JARs com 'scala-library' encontrados no classpath:\n",
      "\n",
      "- /opt/spark/jars/scala-library-2.13.16.jar\n"
     ]
    }
   ],
   "source": [
    "# Acessa o classpath da JVM via SparkContext\n",
    "classpath = spark.sparkContext._jvm.System.getProperty(\"java.class.path\")\n",
    "\n",
    "# Divide os caminhos (separados por ':' no Linux/macOS, ou ';' no Windows)\n",
    "import os\n",
    "sep = \";\" if os.name == \"nt\" else \":\"\n",
    "jars = [path for path in classpath.split(sep) if path.strip().endswith(\".jar\")]\n",
    "\n",
    "# Filtra JARs que contenham \"delta\" no nome (case-insensitive)\n",
    "delta_jars = [jar for jar in jars if \"sas\" in jar.lower()]\n",
    "antlr_jars = [jar for jar in jars if \"parso\" in jar.lower()]\n",
    "scala_legacy = [jar for jar in jars if \"scala-library\" in jar.lower()]\n",
    "\n",
    "# Exibe os JARs encontrados\n",
    "print(f\"{len(delta_jars)} JARs com 'sas' encontrados no classpath:\\n\")\n",
    "for jar in delta_jars:\n",
    "    print(\"-\", jar)\n",
    "print(f\"\\n{len(antlr_jars)} JARs com 'parso' encontrados no classpath:\\n\")\n",
    "for jar in antlr_jars:\n",
    "    print(\"-\", jar)\n",
    "print(f\"\\n{len(scala_legacy)} JARs com 'scala-library' encontrados no classpath:\\n\")\n",
    "for jar in scala_legacy:\n",
    "    print(\"-\", jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "204478fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo SAS encontrado em: /home/sparkuser/app/src/datetime.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# --- 10.2. Definir caminho do arquivo SAS e verificar existência ---\n",
    "sas_path = \"/home/sparkuser/app/src/datetime.sas7bdat\"\n",
    "\n",
    "if not os.path.exists(sas_path):\n",
    "    raise FileNotFoundError(f\"O arquivo SAS não foi encontrado em: {sas_path}\")\n",
    "else:\n",
    "    print(f\"Arquivo SAS encontrado em: {sas_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "494a61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo copiado para o HDFS em: hdfs://namenode:9000/tmp/datetime.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "# Copia o arquivo local SAS para o HDFS para leitura posterior\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_conf)\n",
    "hdfs_sas_path = \"hdfs://namenode:9000/tmp/datetime.sas7bdat\"\n",
    "path_hdfs_sas = spark._jvm.org.apache.hadoop.fs.Path(hdfs_sas_path)\n",
    "\n",
    "# Copia apenas se ainda não existir no HDFS\n",
    "if not fs.exists(path_hdfs_sas):\n",
    "    fs.copyFromLocalFile(False, True, spark._jvm.org.apache.hadoop.fs.Path(sas_path), path_hdfs_sas)\n",
    "    print(f\"Arquivo copiado para o HDFS em: {hdfs_sas_path}\")\n",
    "else:\n",
    "    print(f\"O arquivo já existe no HDFS em: {hdfs_sas_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a4a8820d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 hdfs supergroup       5120 2025-06-14 23:51 hdfs://namenode:9000/tmp/datetime.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls hdfs://namenode:9000/tmp/datetime.sas7bdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "36db05e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_path =\"hdfs://namenode:9000/tmp/datetime.sas7bdat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34bdfce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 4.0.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: py4j\n",
      "Required-by: delta-spark\n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "76373f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: sas7bdat\n",
      "Version: 2.2.3\n",
      "Summary: A sas7bdat file reader for Python\n",
      "Home-page: https://bitbucket.org/jaredhobbs/sas7bdat\n",
      "Author: Jared Hobbs\n",
      "Author-email: jared@pyhacker.com\n",
      "License: MIT\n",
      "Location: /usr/local/lib/python3.10/dist-packages\n",
      "Requires: six\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show sas7bdat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd2b8cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema inferido do SAS ===\n",
      "VAR1    datetime64[ns]\n",
      "VAR2            object\n",
      "VAR3            object\n",
      "VAR4           float64\n",
      "VAR5            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sas7bdat import SAS7BDAT\n",
    "import pandas as pd\n",
    "\n",
    "with SAS7BDAT('/home/sparkuser/app/src/datetime.sas7bdat') as reader:\n",
    "    df_sas = reader.to_data_frame()\n",
    "\n",
    "print(\"=== Schema inferido do SAS ===\")\n",
    "print(df_sas.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cbfde2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 VAR1        VAR2        VAR3     VAR4      VAR5\n",
      "0 2015-02-02 14:42:12  2015-02-02  2015-02-02  20121.0  14:42:12\n",
      "1 2014-01-01 10:14:23  2014-01-01  2014-01-01  19724.0  10:14:23\n",
      "2 2015-01-15 06:15:22  2015-06-15  2015-06-15  20254.0  06:15:22\n",
      "3 1948-09-09 21:32:00  1948-09-16  1948-09-16  -4124.0  21:32:00\n"
     ]
    }
   ],
   "source": [
    "print(df_sas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b84af4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VAR1: timestamp (nullable = true)\n",
      " |-- VAR2: date (nullable = true)\n",
      " |-- VAR3: date (nullable = true)\n",
      " |-- VAR4: double (nullable = true)\n",
      " |-- VAR5: string (nullable = true)\n",
      "\n",
      "+-------------------+----------+----------+-------+--------+\n",
      "|               VAR1|      VAR2|      VAR3|   VAR4|    VAR5|\n",
      "+-------------------+----------+----------+-------+--------+\n",
      "|2015-02-02 14:42:12|2015-02-02|2015-02-02|20121.0|14:42:12|\n",
      "|2014-01-01 10:14:23|2014-01-01|2014-01-01|19724.0|10:14:23|\n",
      "|2015-01-15 06:15:22|2015-06-15|2015-06-15|20254.0|06:15:22|\n",
      "|1948-09-09 21:32:00|1948-09-16|1948-09-16|-4124.0|21:32:00|\n",
      "+-------------------+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converte colunas datetime.time para string antes de criar o Spark DataFrame\n",
    "import datetime\n",
    "\n",
    "for col in df_sas.columns:\n",
    "\tif df_sas[col].apply(lambda x: isinstance(x, datetime.time)).any():\n",
    "\t\tdf_sas[col] = df_sas[col].astype(str)\n",
    "\n",
    "df_sas_spark = spark.createDataFrame(df_sas)\n",
    "df_sas_spark.printSchema()\n",
    "df_sas_spark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c57e550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/14 23:51:49 WARN HiveConf: HiveConf of name hive.server2.thrift.jvm.args does not exist\n",
      "25/06/14 23:51:49 WARN HiveConf: HiveConf of name hive.server2.thrift.java.port does not exist\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 10.4. Criar/usar um database no Hive e gravar os dados SAS em uma tabela Hive (Parquet) ---\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS teste_db\")\n",
    "spark.sql(\"USE teste_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8b07328b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 10.4.1. (Opcional) Verificar se já existe alguma tabela chamada ‘tabela_sas_hive’ e apagar caso exista\n",
    "spark.sql(\"DROP TABLE IF EXISTS tabela_sas_hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b6327487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conteúdo da tabela Hive ‘teste_db.tabela_sas_hive’ ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# 10.4.2. Gravar o DataFrame SAS em uma tabela Hive gerenciada chamada ‘tabela_sas_hive’ (formato Parquet padrão)\n",
    "df_sas_spark.write \\\n",
    "      .mode(\"overwrite\") \\\n",
    "      .format(\"parquet\") \\\n",
    "      .saveAsTable(\"tabela_sas_hive\")\n",
    "\n",
    "# --- 10.5. Consultar o conteúdo da tabela Hive para confirmar gravação ---\n",
    "print(\"=== Conteúdo da tabela Hive ‘teste_db.tabela_sas_hive’ ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9e81e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conteúdo da tabela Hive ‘teste_db.tabela_sas_hive’ ===\n",
      "+-------------------+----------+----------+-------+--------+\n",
      "|VAR1               |VAR2      |VAR3      |VAR4   |VAR5    |\n",
      "+-------------------+----------+----------+-------+--------+\n",
      "|2015-02-02 14:42:12|2015-02-02|2015-02-02|20121.0|14:42:12|\n",
      "|2015-01-15 06:15:22|2015-06-15|2015-06-15|20254.0|06:15:22|\n",
      "|1948-09-09 21:32:00|1948-09-16|1948-09-16|-4124.0|21:32:00|\n",
      "|2014-01-01 10:14:23|2014-01-01|2014-01-01|19724.0|10:14:23|\n",
      "+-------------------+----------+----------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 10.5. Consultar o conteúdo da tabela Hive para confirmar gravação ---\n",
    "print(\"=== Conteúdo da tabela Hive ‘teste_db.tabela_sas_hive’ ===\")\n",
    "spark.sql(\"SELECT * FROM teste_db.tabela_sas_hive\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9eb8f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros gravados em ‘tabela_sas_hive’: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 10.6. Mostrar contagem total de registros para confirmar que todos foram salvos ---\n",
    "total_registros = spark.sql(\"SELECT COUNT(*) AS cnt FROM teste_db.tabela_sas_hive\").collect()[0][\"cnt\"]\n",
    "print(f\"Total de registros gravados em ‘tabela_sas_hive’: {total_registros}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
