#  Modern Data Stack Delta-Spark-Hive

**Esse projeto mira criar uma plataforma em bigdata para estudos, s√£o criados diversos servi√ßos no docker como um ambiente provisionado e s√£o estruturadas conex√µes entre os ambientes.**

o projeto foi inspirado na ideia de ter um ambiente de laborat√≥rio com spark 4 e airflow 3. 

Principais servi√ßos do ambiente:

| Componente | Vers√£o   |
|------------|----------|
| Spark      | 4.0.0    |
| Scala      | 2.13     |
| Delta      | 4.0.1    |
| Airflow    | 3.0      |
| Hive       | 4.0.1    |


## Arquitetura do Projeto

<img src="arquitetura.png" alt="Arquitetura do Projeto">


---

---
# Como iniciar o projeto?

> ‚ö†Ô∏è **Aten√ß√£o:** o projeto foi configurado para uso do python 3.10.x para compatibilidade entre o airflow worker e o spark, que serve para jupyter e spark master no mesmo cont√¢iner, recomenda-se n√£o fazer **downgrade** de vers√µes do python, perceba que, ao mudar a vers√£o do python no spark, devemos mudar o python no airflow e vice-versa.
>> o *compose* foi configurado para persistir dados localmente, para n√£o ocorrer deletes de volumes acidentais em outros envs que o usu√°rio pode configurar. sinta-se a vontade para alterar isso no docker compose se preferir. 

## Iniciando de fato o projeto

1. inicie o projeto 

    git clone https://github.com/ibmendes/mds-spark4

> na primeira execu√ß√£o, executar no terminal 'docker compose build' em seguida, podemos subir o projeto pelo bash

    ```bash
    chmod +x start.sh
    ./start.sh
    ou 
    sh start.sh
    ```

2. lidando com o docker

    para derrubar o projeto, utilize:

        CTRL+C no terminal bash aberto / docker compose down

    para apagar os dados do projeto utilize

        docker compose down -v

    iniciando servi√ßos indivdualmente

        docker compose up <servi√ßo>

        ou pelo docker compose no vscode, clicando em run service:

---
## üì¶ Servi√ßos e Portas

| Servi√ßo                | Porta                | Descri√ß√£o                                   | Link de Acesso                                      |
|------------------------|----------------------|---------------------------------------------|-----------------------------------------------------|
| PostgreSQL             | `5432`               | Backend Airflow, Hive Metastore etc         | N/A (acesso via cliente SQL, ex: DBeaver)           |
| HiveServer2            | `10000`              | JDBC/Beeline                                | beeline -u jdbc:hive2://localhost:10000 ou beeline  |
| Metastore              | `9863` / `10002`     | API Thrift para Metastore                   | N/A (acesso via Spark/Hive)                         |
| NameNode               | `9870`               | Interface Web do HDFS NameNode              | [http://localhost:9870](http://localhost:9870)      |
| RPC HDFS NameNode      | `9000`               | Comunica√ß√£o entre servi√ßos HDFS             | hdfs://namenode:9000/                               |
| DataNode               | `9864`               | Interface Web do HDFS DataNode              | [http://localhost:9864](http://localhost:9864)      |
| Airflow 3.0 api-server | `8080`               | Interface Web do Airflow                    | [http://localhost:8080](http://localhost:8080)      |
| Flower                 | `5555` / `5558`      | Depend√™ncia do Airflow, Celery Worker       | n/a                                                 |
| Redis                  | `6379`               | Depend√™ncia do Airflow, backend             | N/A                                                 |
| Spark UI               |`7078`/`4040`(session)| Interface Web do Spark (ativa na sess√£o)    | [http://localhost:7078](http://localhost:7078)      |
| trino                  | `9090`               | ui presto/trino para o superset/modelagem   | [http://localhost:9090](http://localhost:9090)      |
| mongo-express          | `9092`               |  nosql                                      | [http://localhost:9092](http://localhost:9092)      |
| nifi                   | `8443`               |                                             | [https://localhost:8443/nifi](https://localhost:8443/nifi)|


üìà **Data Viz**

| Servi√ßo   | Porta   | Descri√ß√£o                                 | Link de Acesso                                 |
|-----------|---------|-------------------------------------------|------------------------------------------------|
| Hue       | `8889`  | Consultas SQL a databases                 | [http://localhost:8889](http://localhost:8889) |
| superset  | `9091`  | Consultas SQL a databases e dashboards                 | [http://localhost:9091](http://localhost:9091) |
---

üì¶ Credenciais de acesso entre os servi√ßos

| Servi√ßo        | Host       | Porta  | Banco de Dados      | Usu√°rio           | Senha        |
|----------------|------------|--------|---------------------|-------------------|--------------|
| PostgreSQL     | localhost  | 5432   | metastore           | hiveuser          | hivepassword |
| Airflow        | localhost  | 8080   | airflow (Postgres)  | airflow           | airflow      |
| Spark          | localhost  | 8081   | ‚Äî                   | sparkuser         | ‚Äî            |
| Spark Master   | localhost  | 7077   | ‚Äî                   | sparkuser         | ‚Äî            |
| Hue            | localhost  | 8889   | ‚Äî                   | admin             | admin        |
| Hive Metastore | localhost  | 10000  | default             | hdfs              | ‚Äî            |
| trino          | localhost  | 9090   | -                   | admin             | ‚Äî            |
| superset       | localhost  | 9091   | -                   | admin             |admin         |
| mongo-express  | localhost  | 8081   | -                   | root              |example       |
| nifi           | localhost  | 8443   | -                   | admin             |adminadmin123 |

üîç Acesso via terminal ao **hive (estando no Cont√¢iner do postgre)**:

```bash
psql -h localhost -U hiveuser -d metastore
```



# Entendendo a estrutura do projeto

    ‚îú‚îÄ‚îÄ arquitetura.png
    ‚îú‚îÄ‚îÄ dbt
    ‚îú‚îÄ‚îÄ docker-compose.yml
    ‚îú‚îÄ‚îÄ notebooks <--- qualquer notebook ou py aqui ir√° pro jspark
    ‚îú‚îÄ‚îÄ readme.md
    ‚îú‚îÄ‚îÄ src-docker  <- estrutura pertinente ao docker, "servico.Dockerfile" √© incluso na raiz para facilitar o contexto nas subpastas
    ‚îÇ   ‚îú‚îÄ‚îÄ {servico}.Dockerfile
    ‚îÇ   ‚îú‚îÄ‚îÄ build
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config <-- config hive
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ downloads <------------------------------imagens de insumo para criar o ambiente hive e hdfs
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt  <-- pypi
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ jars <--- jars extras para o classpath spark
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ambiente <- jars de ambiente
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ superset
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trino
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ etc
    ‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ catalog
    ‚îÇ   ‚îÇ           ‚îÇ   ‚îî‚îÄ‚îÄ hive.properties <-- acesso ao hive metastore
    ‚îÇ   ‚îú‚îÄ‚îÄ setup.sh <---------- download tarballs
    ‚îú‚îÄ‚îÄ stack <--------- onde v√£o as coisas em um ambiente produtivo
    ‚îÇ   ‚îú‚îÄ‚îÄ airflow
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dags <--- espa√ßos para versionar as dags do airflow
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ notebooks <---- para servir como utils de dags com papermill
    ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ py  <---- para servir como utils de dags nativas em py
    ‚îÇ   ‚îú‚îÄ‚îÄ notebooks	<--- espa√ßo para testes, servindo como uma outra alternativa da /notebooks da raiz do projeto. 
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ saslab.ipynb
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ teste_ambiente.ipynb
    ‚îÇ   ‚îî‚îÄ‚îÄ trino
    ‚îÇ       ‚îî‚îÄ‚îÄ teste.sql <- querys de exemplo pra uso no container do trino
    ‚îî‚îÄ‚îÄ start.sh <- init

## Documenta√ß√µes
Airflow:
- [Airflow Docker Compose Guide](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html)  
- [Building Airflow Docker Stack](https://airflow.apache.org/docs/docker-stack/build.html)


### Hive
- [Hive Quickstart Guide](https://hive.apache.org/developement/quickstart/)

### Postgres Documentation
- [Postgres Docker Hub](https://hub.docker.com/_/postgres)

### spark 4.0.0  + delta-spark
- [releases apache spark](https://spark.apache.org/releases/spark-release-4-0-0.html)
- [delta with spark 4](https://github.com/delta-io/delta/releases/tag/v4.0.0-final-rc1)

### trino
- [hdfs configuration](https://trino.io/docs/current/connector/hive.html#hive-file-system-configuration)

### dbt
- [docker-install dbt](https://docs.getdbt.com/docs/core/docker-install)

### mongodb express
- [Express & MongoDB](https://www.mongodb.com/resources/products/compatibilities/express)

### nifi
- [The core concepts of NiFi](https://nifi.apache.org/docs/nifi-docs/)


## Refer√™ncias e Cr√©ditos
O Contexto hive + hdfs foi construido partindo do fork do repos√≥t√≥rio abaixo:
- [spark_delta_hive_metastore](https://github.com/experientlabs/spark_delta_hive_metastore)

> ‚ö†Ô∏è **Disclaimers relevantes:** por se tratar de um ambiente de laborat√≥rio, s√£o removidas autentica√ß√µes do servi√ßo e grande parte das pol√≠ticas de seguran√ßa, a finalidade desse cluster √© ser um ambiente de estudos e n√£o produtivo.